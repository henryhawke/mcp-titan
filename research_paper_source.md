
26
References
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. â€œGpt-4 technical reportâ€. In: arXiv preprint
arXiv:2303.08774 (2023).
[2] Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, and
Daniil Gavrilov. â€œLinear Transformers with Learnable Kernel Functions are Better In-Context Modelsâ€. In: arXiv
preprint arXiv:2402.10644 (2024).
[3] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan
Shillingford, and Nando De Freitas. â€œLearning to learn by gradient descent by gradient descentâ€. In: Advances in
neural information processing systems 29 (2016).
[4] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone,
Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. â€œExploring length generalization in large language modelsâ€. In:
Advances in Neural Information Processing Systems 35 (2022), pp. 38546â€“38556.
[5] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christo-
pher Re. â€œSimple linear attention language models balance the recall-throughput tradeoff â€. In: Forty-first International
Conference on Machine Learning. 2024. url: https://openreview.net/forum?id=e93ffDcpH3.
[6] Dzmitry Bahdanau. â€œNeural machine translation by jointly learning to align and translateâ€. In: arXiv preprint
arXiv:1409.0473 (2014).
[7] Reza Bayat, Mohammad Pezeshki, Elvis Dohmatob, David Lopez-Paz, and Pascal Vincent. â€œThe Pitfalls of Memo-
rization: When Memorization Hurts Generalizationâ€. In: arXiv preprint arXiv:2412.07684 (2024).
[8] Maximilian Beck, Korbinian PÃ¶ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp,
GÃ¼nter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. â€œxLSTM: Extended Long Short-Term Memoryâ€. In:
arXiv preprint arXiv:2405.04517 (2024).
[9] Ali Behrouz, Michele Santacatterina, and Ramin Zabih. â€œMambamixer: Efficient selective state space models with
dual token and channel selectionâ€. In: arXiv preprint arXiv:2403.19888 (2024).
[10] Vincent-Pierre Berges, Barlas OÄŸuz, Daniel Haziza, Wen-tau Yih, Luke Zettlemoyer, and Gargi Gosh. â€œMemory
Layers at Scaleâ€. In: arXiv preprint arXiv:2412.09764 (2024).
[11] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. â€œBirth of a transformer: A
memory viewpointâ€. In: Advances in Neural Information Processing Systems 36 (2024).
[12] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. â€œPiqa: Reasoning about physical commonsense in
natural languageâ€. In: Proceedings of the AAAI conference on artificial intelligence. Vol. 34. 05. 2020, pp. 7432â€“7439.
[13] Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard
Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, et al. â€œRecurrentGemma: Moving Past Transformers
for Efficient Open Language Modelsâ€. In: arXiv preprint arXiv:2404.07839 (2024).
[14] LÃ©on Bottou and Vladimir Vapnik. â€œLocal learning algorithmsâ€. In: Neural computation 4.6 (1992), pp. 888â€“900.
[15] Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, and Mikhail S Burtsev. â€œScaling transformer to 1m tokens and
beyond with rmtâ€. In: arXiv preprint arXiv:2304.11062 (2023).
[16] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. â€œRecurrent memory transformerâ€. In: Advances in Neural
Information Processing Systems 35 (2022), pp. 11079â€“11091.
[17] Edoardo Cetin, Qi Sun, Tianyu Zhao, and Yujin Tang. â€œAn Evolved Universal Transformer Memoryâ€. In: arXiv
preprint arXiv:2410.13166 (2024).
[18] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher RÃ©. â€œScatterbrain: Unifying sparse and
low-rank attentionâ€. In: Advances in Neural Information Processing Systems 34 (2021), pp. 17413â€“17426.
[19] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,
Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and
Adrian Weller. â€œRethinking Attention with Performersâ€. In: International Conference on Learning Representations.
2021. url: https://openreview.net/forum?id=Ua6zuk0WRH.
[20] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
â€œBoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questionsâ€. In: Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers). Ed. by Jill Burstein, Christy Doran, and Thamar Solorio. Minneapolis, Minnesota:
Association for Computational Linguistics, June 2019, pp. 2924â€“2936. doi: 10.18653/v1/N19-1300. url: https:
//aclanthology.org/N19-1300/.
18
[21] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Taf jord.
â€œThink you have solved question answering? try arc, the ai2 reasoning challengeâ€. In: arXiv preprint arXiv:1803.05457
(2018).
[22] Nelson Cowan. â€œWhat are the differences between long-term, short-term, and working memory?â€ In: Progress in
brain research 169 (2008), pp. 323â€“338.
[23] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. â€œTransformer-
XL: Attentive Language Models beyond a Fixed-Length Contextâ€. In: ACL (1). Ed. by Anna Korhonen, David R.
Traum, and LluÃ­s MÃ rquez. Association for Computational Linguistics, 2019, pp. 2978â€“2988. isbn: 978-1-950737-48-2.
[24] Tri Dao. â€œFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioningâ€. In: The Twelfth Inter-
national Conference on Learning Representations. 2024. url: https://openreview.net/forum?id=mZn2Xyh9Ec.
[25] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. â€œFlashAttention: Fast and Memory-Efficient
Exact Attention with IO-Awarenessâ€. In: Advances in Neural Information Processing Systems. Ed. by S. Koyejo, S.
Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., 2022, pp. 16344â€“16359. url:
https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-
Paper-Conference.pdf.
[26] Tri Dao and Albert Gu. â€œTransformers are SSMs: Generalized models and efficient algorithms through structured
state space dualityâ€. In: arXiv preprint arXiv:2405.21060 (2024).
[27] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K Mathur, Rajat Sen, and Rose Yu. â€œLong-term Forecasting
with TiDE: Time-series Dense Encoderâ€. In: Transactions on Machine Learning Research (2023). issn: 2835-8856. url:
https://openreview.net/forum?id=pCbC3aQB5W.
[28] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba
Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. â€œGriffin: Mixing gated linear recurrences with
local attention for efficient language modelsâ€. In: arXiv preprint arXiv:2402.19427 (2024).
[29] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. â€œFlex Attention: A Programming Model
for Generating Optimized Attention Kernelsâ€. In: arXiv preprint arXiv:2412.05496 (2024).
[30] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu,
Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. â€œHymba: A Hybrid-head Architecture for Small
Language Modelsâ€. In: arXiv preprint arXiv:2411.13676 (2024).
[31] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. â€œSigmoid-weighted linear units for neural network function approxi-
mation in reinforcement learningâ€. In: Neural networks 107 (2018), pp. 3â€“11.
[32] Yukun Feng, Feng Li, Ziang Song, Boyuan Zheng, and Philipp Koehn. â€œLearn to remember: Transformer with
recurrent memory for document-level machine translationâ€. In: arXiv preprint arXiv:2205.01546 (2022).
[33] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. â€œHungry Hungry
Hippos: Towards Language Modeling with State Space Modelsâ€. In: The Eleventh International Conference on Learning
Representations. 2023. url: https://openreview.net/forum?id=COZDy0WYGg.
[34] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. â€œTest-time training with masked autoencodersâ€. In:
Advances in Neural Information Processing Systems 35 (2022), pp. 29374â€“29385.
[35] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,
Anish Thite, Noa Nabeshima, et al. â€œThe pile: An 800gb dataset of diverse text for language modelingâ€. In: arXiv
preprint arXiv:2101.00027 (2020).
[36] Felix A Gers, JÃ¼rgen Schmidhuber, and Fred Cummins. â€œLearning to forget: Continual prediction with LSTMâ€. In:
Neural computation 12.10 (2000), pp. 2451â€“2471.
[37] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. 2014. arXiv: 1410.5401 [cs.NE]. url:
https://arxiv.org/abs/1410.5401.
[38] Klaus Greff, Rupesh K Srivastava, Jan KoutnÃ­k, Bas R Steunebrink, and JÃ¼rgen Schmidhuber. â€œLSTM: A search space
odysseyâ€. In: IEEE transactions on neural networks and learning systems 28.10 (2016), pp. 2222â€“2232.
[39] KatarÃ­na GreÅ¡ovÃ¡, Vlastimil Martinek, David ÄŒechÃ¡k, Petr Å imeÄek, and Panagiotis Alexiou. â€œGenomic benchmarks:
a collection of datasets for genomic sequence classificationâ€. In: BMC Genomic Data 24.1 (2023), p. 25.
[40] Albert Gu and Tri Dao. â€œMamba: Linear-Time Sequence Modeling with Selective State Spacesâ€. In: First Conference
on Language Modeling. 2024. url: https://openreview.net/forum?id=tEYskw1VY2.
[41] Albert Gu, Karan Goel, and Christopher Re. â€œEfficiently Modeling Long Sequences with Structured State Spacesâ€.
In: International Conference on Learning Representations. 2022. url: https://openreview.net/forum?id=
uYLFoz1vlAC.
19
[42] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. â€œLM-Infinite: Zero-Shot
Extreme Length Generalization for Large Language Modelsâ€. In: Proceedings of the 2024 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long
Papers). Ed. by Kevin Duh, Helena Gomez, and Steven Bethard. Mexico City, Mexico: Association for Computational
Linguistics, June 2024, pp. 3991â€“4008. doi: 10.18653/v1/2024.naacl-long.222. url: https://aclanthology.
org/2024.naacl-long.222.
[43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. â€œLiquid
Structural State-Space Modelsâ€. In: The Eleventh International Conference on Learning Representations. 2023. url:
https://openreview.net/forum?id=g4OTKRKfS7R.
[44] Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, and Rogerio Feris. â€œCAMELoT:
Towards Large Language Models with Training-Free Consolidated Associative Memoryâ€. In: arXiv preprint
arXiv:2402.13449 (2024).
[45] Donald Olding Hebb. The organization of behavior: A neuropsychological theory. Psychology press, 2005.
[46] John J Hopfield. â€œNeural networks and physical systems with emergent collective computational abilities.â€ In:
Proceedings of the national academy of sciences 79.8 (1982), pp. 2554â€“2558.
[47] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. â€œMultilayer feedforward networks are universal approxi-
matorsâ€. In: Neural networks 2.5 (1989), pp. 359â€“366.
[48] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg.
â€œRULER: Whatâ€™s the Real Context Size of Your Long-Context Language Models?â€ In: First Conference on Language
Modeling. 2024. url: https://openreview.net/forum?id=kIoBbc76Sy.
[49] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. â€œBlock-recurrent transformersâ€.
In: Advances in neural information processing systems 35 (2022), pp. 33248â€“33261.
[50] Kazuki Irie, RÃ³bert CsordÃ¡s, and JÃ¼rgen Schmidhuber. â€œThe dual form of neural networks revisited: Connecting test
time predictions to training patterns via spotlights of attentionâ€. In: International Conference on Machine Learning.
PMLR. 2022, pp. 9639â€“9659.
[51] Kazuki Irie, Imanol Schlag, RÃ³bert CsordÃ¡s, and JÃ¼rgen Schmidhuber. â€œGoing beyond linear transformers with
recurrent fast weight programmersâ€. In: Advances in neural information processing systems 34 (2021), pp. 7703â€“7717.
[52] Vidit Jain and Erik Learned-Miller. â€œOnline domain adaptation of a pre-trained cascade of classifiersâ€. In: CVPR
2011. IEEE. 2011, pp. 577â€“584.
[53] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las
Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. â€œMistral 7Bâ€. In: arXiv preprint
arXiv:2310.06825 (2023).
[54] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. â€œPolySketchFormer: Fast Transformers via Sketching Polyno-
mial Kernelsâ€. In: Forty-first International Conference on Machine Learning. 2024. url: https://openreview.net/
forum?id=ghYrfdJfjK.
[55] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford, Jeffrey Wu, and Dario Amodei. â€œScaling laws for neural language modelsâ€. In: arXiv preprint arXiv:2001.08361
(2020).
[56] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. â€œTransformers are rnns: Fast au-
toregressive transformers with linear attentionâ€. In: International conference on machine learning. PMLR. 2020,
pp. 5156â€“5165.
[57] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. â€œGeneralization through
Memorization: Nearest Neighbor Language Modelsâ€. In: International Conference on Learning Representations. 2020.
url: https://openreview.net/forum?id=HklBjCEKvH.
[58] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Igorevich Sorokin, Artyom Sorokin, and Mikhail
Burtsev. â€œBABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystackâ€. In: The Thirty-
eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024. url: https:
//openreview.net/forum?id=u7m2CG84BQ.
[59] Hung Le, Truyen Tran, and Svetha Venkatesh. â€œSelf-attentive associative memoryâ€. In: International conference on
machine learning. PMLR. 2020, pp. 5682â€“5691.
[60] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler,
Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et al. â€œRetrieval-augmented generation for knowledge-intensive nlp
tasksâ€. In: Advances in Neural Information Processing Systems 33 (2020), pp. 9459â€“9474.
20
[61] Danny Leybzon and Corentin Kervadec. â€œLearning, Forgetting, Remembering: Insights From Tracking LLM Mem-
orization During Trainingâ€. In: Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural
Networks for NLP. 2024, pp. 43â€“57.
[62] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. â€œRevisiting long-term time series forecasting: An investigation on linear
mappingâ€. In: arXiv preprint arXiv:2305.10721 (2023).
[63] Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. â€œLonghorn: State space models are amortized
online learnersâ€. In: arXiv preprint arXiv:2407.14207 (2024).
[64] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
â€œLost in the middle: How language models use long contextsâ€. In: Transactions of the Association for Computational
Linguistics 12 (2024), pp. 157â€“173.
[65] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. â€œitransformer:
Inverted transformers are effective for time series forecastingâ€. In: arXiv preprint arXiv:2310.06625 (2023).
[66] George Mandler. â€œThe structure of value: Accounting for tasteâ€. In: Affect and cognition. Psychology Press, 2014,
pp. 3â€“36.
[67] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. â€œLong Range Language Modeling via
Gated State Spacesâ€. In: The Eleventh International Conference on Learning Representations. 2023. url: https:
//openreview.net/forum?id=5MkYIYCbva.
[68] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. â€œPointer Sentinel Mixture Modelsâ€. In:
International Conference on Learning Representations. 2017. url: https://openreview.net/forum?id=Byj72udxe.
[69] William Merrill, Jackson Petty, and Ashish Sabharwal. â€œThe Illusion of State in State-Space Modelsâ€. In: Forty-first
International Conference on Machine Learning. 2024. url: https://openreview.net/forum?id=QZgo9JZpLq.
[70] Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, and Kayvon Fatahalian. â€œOnline model distillation
for efficient video inferenceâ€. In: Proceedings of the IEEE/CVF International conference on computer vision. 2019,
pp. 3573â€“3582.
[71] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. â€œLeave no context behind: Efficient infinite context
transformers with infini-attentionâ€. In: arXiv preprint arXiv:2404.07143 (2024).
[72] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. â€œMetalearned neural memoryâ€. In:
Advances in Neural Information Processing Systems 32 (2019).
[73] Tsendsuren Munkhdalai and Hong Yu. â€œNeural semantic encodersâ€. In: Proceedings of the conference. Association for
Computational Linguistics. Meeting. Vol. 1. NIH Public Access. 2017, p. 397.
[74] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli,
Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. â€œHyenadna: Long-range genomic sequence modeling at single
nucleotide resolutionâ€. In: Advances in neural information processing systems 36 (2024).
[75] A Nichol. â€œOn first-order meta-learning algorithmsâ€. In: arXiv preprint arXiv:1803.02999 (2018).
[76] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. â€œA time series is worth 64 words:
Long-term forecasting with transformersâ€. In: arXiv preprint arXiv:2211.14730 (2022).
[77] Hideyuki Okano, Tomoo Hirano, and Evan Balaban. â€œLearning and memoryâ€. In: Proceedings of the National Academy
of Sciences 97.23 (2000), pp. 12403â€“12404.
[78] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.
â€œResurrecting recurrent neural networks for long sequencesâ€. In: International Conference on Machine Learning.
PMLR. 2023, pp. 26670â€“26698.
[79] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle,
Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez. â€œThe LAMBADA dataset: Word prediction requiring a broad
discourse contextâ€. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). Ed. by Katrin Erk and Noah A. Smith. Berlin, Germany: Association for Computational Linguistics,
Aug. 2016, pp. 1525â€“1534. doi: 10.18653/v1/P16-1144. url: https://aclanthology.org/P16-1144/.
[80] Badri N. Patro and Vijay S. Agneeswaran. SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate
Time series. 2024. arXiv: 2403.15360 [cs.CV].
[81] Guilherme Penedo, Hynek KydlÃ­Äek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro
Von Werra, and Thomas Wolf. â€œThe FineWeb Datasets: Decanting the Web for the Finest Text Data at Scaleâ€. In:
The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024. url:
https://openreview.net/forum?id=n6SCkn2QaG.
[82] Bo Peng. RWKV-LM. Version 1.0.0. Aug. 2021. doi: 10.5281/zenodo.5196577. url: https://github.com/
BlinkDL/RWKV-LM.
21
[83] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao,
Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He,
Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, BartÅ‚omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna
Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, StanisÅ‚aw WoÅºniak,
Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. â€œRWKV: Reinventing RNNs for the Transformer Eraâ€.
In: The 2023 Conference on Empirical Methods in Natural Language Processing. 2023. url: https://openreview.
net/forum?id=7SaXczaBpG.
[84] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian
Du, Teddy Ferdinan, Haowen Hou, et al. â€œEagle and finch: Rwkv with matrix-valued states and dynamic recurrenceâ€.
In: arXiv preprint arXiv:2404.05892 (2024).
[85] DL Prados and SC Kak. â€œNeural network capacity using delta ruleâ€. In: Electronics Letters 25.3 (1989), pp. 197â€“199.
[86] Zhen Qin, Yiran Zhong, and Hui Deng. â€œExploring Transformer Extrapolationâ€. In: Proceedings of the AAAI
Conference on Artificial Intelligence. Vol. 38. 17. 2024, pp. 18897â€“18905.
[87] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. â€œSamba: Simple Hybrid State Space
Models for Efficient Unlimited Context Language Modelingâ€. In: arXiv preprint arXiv:2406.07522 (2024).
[88] Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, and Mikhail Burtsev. â€œAssociative recurrent memory transformerâ€. In:
arXiv preprint arXiv:2407.04841 (2024).
[89] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. â€œEfficient content-based sparse attention with
routing transformersâ€. In: Transactions of the Association for Computational Linguistics 9 (2021), pp. 53â€“68.
[90] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. â€œWinogrande: An adversarial winograd
schema challenge at scaleâ€. In: Communications of the ACM 64.9 (2021), pp. 99â€“106.
[91] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. â€œSocial IQa: Commonsense Reasoning
about Social Interactionsâ€. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Ed. by Kentaro Inui,
Jing Jiang, Vincent Ng, and Xiaojun Wan. Hong Kong, China: Association for Computational Linguistics, Nov. 2019,
pp. 4463â€“4473. doi: 10.18653/v1/D19-1454. url: https://aclanthology.org/D19-1454/.
[92] Imanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber. â€œLinear transformers are secretly fast weight programmersâ€.
In: International Conference on Machine Learning. PMLR. 2021, pp. 9355â€“9366.
[93] JH Schmidhuber. â€œLearning to control fast-weight memories: An alternative to recurrent nets. Accepted for
publication inâ€. In: Neural Computation (1992).
[94] JÃ¼rgen Schmidhuber. â€œReducing the ratio between learning complexity and number of time varying variables
in fully recurrent netsâ€. In: ICANNâ€™93: Proceedings of the International Conference on Artificial Neural Networks
Amsterdam, The Netherlands 13â€“16 September 1993 3. Springer. 1993, pp. 460â€“463.
[95] JÃ¼rgen Schmidhuber and Sepp Hochreiter. â€œLong Short-term Memoryâ€. In: Neural Computation MIT-Press (1997).
[96] Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C Lipton, and J Zico Kolter. â€œRethinking llm memorization
through the lens of adversarial compressionâ€. In: arXiv preprint arXiv:2404.15146 (2024).
[97] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. â€œSimplified State Space Layers for Sequence Modelingâ€.
In: The Eleventh International Conference on Learning Representations. 2023. url: https://openreview.net/forum?
id=Ai8Hw3AXqks.
[98] Robin Staab, Mark Vero, Mislav Balunovic, and Martin Vechev. â€œBeyond Memorization: Violating Privacy via
Inference with Large Language Modelsâ€. In: The Twelfth International Conference on Learning Representations. 2024.
url: https://openreview.net/forum?id=kmn0BhQk7p.
[99] Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. â€œAugmenting self-
attention with persistent memoryâ€. In: arXiv preprint arXiv:1907.01470 (2019).
[100] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. â€œEnd-to-end memory networksâ€. In: Advances in neural
information processing systems 28 (2015).
[101] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong
Wang, Sanmi Koyejo, et al. â€œLearning to (learn at test time): Rnns with expressive hidden statesâ€. In: arXiv preprint
arXiv:2407.04620 (2024).
[102] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. â€œRetentive
network: A successor to transformer for large language modelsâ€. In: arXiv preprint arXiv:2307.08621 (2023).
[103] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,
Morgane RiviÃ¨re, Mihir Sanjay Kale, Juliette Love, et al. â€œGemma: Open models based on gemini research and
technologyâ€. In: arXiv preprint arXiv:2403.08295 (2024).
22
[104] W Scott Terry. Learning and memory: Basic principles, processes, and procedures. Routledge, 2017.
[105] Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco Gori, and Stefano Melacci. â€œOn the
resurgence of recurrent models for long sequences: Survey and research opportunities in the transformer eraâ€. In:
arXiv preprint arXiv:2402.08132 (2024).
[106] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste
RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. â€œLlama: Open and efficient foundation language modelsâ€.
In: arXiv preprint arXiv:2302.13971 (2023).
[107] Jos Van Der Westhuizen and Joan Lasenby. â€œThe unreasonable effectiveness of the forget gateâ€. In: arXiv preprint
arXiv:1804.04849 (2018).
[108] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser,
and Illia Polosukhin. â€œAttention is All you Needâ€. In: Advances in Neural Information Processing Systems. Ed.
by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Cur-
ran Associates, Inc., 2017. url: https : / / proceedings . neurips . cc / paper _ files / paper / 2017 / file /
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
[109] Shida Wang. â€œLongSSM: On the Length Extension of State-space Models in Language Modellingâ€. In: arXiv preprint
arXiv:2406.02080 (2024).
[110] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin,
Jingbo Shang, and Julian McAuley. â€œMEMORYLLM: Towards Self-Updatable Large Language Modelsâ€. In: Forty-first
International Conference on Machine Learning. 2024. url: https://openreview.net/forum?id=p0lKWzdikQ.
[111] Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, Wei Wang,
Gholamreza Haffari, et al. â€œTowards LifeSpan Cognitive Systemsâ€. In: arXiv preprint arXiv:2409.13265 (2024).
[112] Zhiwei Wang, Yao Ma, Zitao Liu, and Jiliang Tang. â€œR-transformer: Recurrent neural network enhanced transformerâ€.
In: arXiv preprint arXiv:1907.05572 (2019).
[113] Jason Weston, Sumit Chopra, and Antoine Bordes. â€œMemory networksâ€. In: arXiv preprint arXiv:1410.3916 (2014).
[114] Bernard Widrow and Marcian E Hoff. â€œAdaptive switching circuitsâ€. In: Neurocomputing: foundations of research.
1988, pp. 123â€“134.
[115] Ronald J Williams and David Zipser. â€œA learning algorithm for continually running fully recurrent neural networksâ€.
In: Neural computation 1.2 (1989), pp. 270â€“280.
[116] Daniel B Willingham. â€œSystems of memory in the human brainâ€. In: Neuron 18.1 (1997), pp. 5â€“8.
[117] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross Girshick. â€œLong-
term feature banks for detailed video understandingâ€. In: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. 2019, pp. 284â€“293.
[118] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. â€œTimesNet: Temporal 2D-
Variation Modeling for General Time Series Analysisâ€. In: The Eleventh International Conference on Learning
Representations. 2023. url: https://openreview.net/forum?id=ju_Uqw384Oq.
[119] Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu. â€œMemformer: A memory-
augmented transformer for sequence modelingâ€. In: arXiv preprint arXiv:2010.06891 (2020).
[120] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. â€œEfficient Streaming Language Models
with Attention Sinksâ€. In: The Twelfth International Conference on Learning Representations. 2024. url: https:
//openreview.net/forum?id=NG7sS51zVF.
[121] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei
Huang, Haoran Wei, et al. â€œQwen2. 5 Technical Reportâ€. In: arXiv preprint arXiv:2412.15115 (2024).
[122] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. â€œGated Delta Networks: Improving Mamba2 with Delta Ruleâ€. In:
arXiv preprint arXiv:2412.06464 (2024).
[123] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. â€œGated Linear Attention Transformers
with Hardware-Efficient Trainingâ€. In: Forty-first International Conference on Machine Learning. 2024. url: https:
//openreview.net/forum?id=ia5XvxFUJT.
[124] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. â€œParallelizing Linear Transformers with the
Delta Rule over Sequence Lengthâ€. In: The Thirty-eighth Annual Conference on Neural Information Processing Systems.
2024. url: https://openreview.net/forum?id=y8Rm4VNRPH.
[125] Luca Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager,
Alessandro Achille, and Stefano Soatto. â€œBâ€™MOJO: Hybrid State Space Realizations of Foundation Models with
Eidetic and Fading Memoryâ€. In: The Thirty-eighth Annual Conference on Neural Information Processing Systems.
2024. url: https://openreview.net/forum?id=RnQdRY1h5v.
23
[126] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. â€œHellaSwag: Can a Machine Really Finish
Your Sentence?â€ In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Ed. by
Anna Korhonen, David Traum, and LluÃ­s MÃ rquez. Florence, Italy: Association for Computational Linguistics, July
2019, pp. 4791â€“4800. doi: 10.18653/v1/P19-1472. url: https://aclanthology.org/P19-1472/.
[127] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. â€œAre transformers effective for time series forecasting?â€ In:
Proceedings of the AAAI conference on artificial intelligence. Vol. 37. 2023, pp. 11121â€“11128.
[128] Hao Zhang, Alexander C Berg, Michael Maire, and Jitendra Malik. â€œSVM-KNN: Discriminative nearest neighbor
classification for visual category recognitionâ€. In: 2006 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPRâ€™06). Vol. 2. IEEE. 2006, pp. 2126â€“2136.
[129] Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan, Beidi Chen, and LÃ©on Bottou. â€œMemory Mosaicsâ€. In: arXiv preprint
arXiv:2405.06394 (2024).
[130] Yunhao Zhang and Junchi Yan. â€œCrossformer: Transformer utilizing cross-dimension dependency for multivariate
time series forecastingâ€. In: The eleventh international conference on learning representations. 2023.
[131] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. â€œInformer:
Beyond efficient transformer for long sequence time-series forecastingâ€. In: Proceedings of the AAAI conference on
artificial intelligence. Vol. 35. 12. 2021, pp. 11106â€“11115.
[132] Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. â€œFast context adaptation via
meta-learningâ€. In: International Conference on Machine Learning. PMLR. 2019, pp. 7693â€“7702.
24
A Related Work
There are diverse perspectives that can independently lead to the design of Titans or its components. Accordingly, to
further situate our work in a broader context, we review three categories of studies:
A.1 Linear Recurrent Models
Recently, to address the computational cost of Transformers in both training and inference, linear recurrent models
have attracted much attention (Tiezzi et al. 2024), mainly due to their fast inference and training. The first generation
of modelsâ€“such as RetNet (Yutao Sun et al. 2023), LRU (Orvieto et al. 2023), RWKV (Peng, Alcaide, et al. 2023), S5 (J. T.
Smith, Warrington, and Linderman 2023), and S4 (Gu, Goel, and Re 2022)â€“uses data-independent transition matrix/decay
mechanism. The second generation of such models started to incorporate gating mechanism, a widely used techniques
in traditional RNNs (Gers, JÃ¼rgen Schmidhuber, and Cummins 2000; Greff et al. 2016; Van Der Westhuizen and Lasenby
2018), into such linear architecturesâ€“e.g., Griffin (De et al. 2024), SSMs (Behrouz, Santacatterina, and Zabih 2024; Dao
and Gu 2024; Gu and Dao 2024; Hasani et al. 2023), RWKV6 (Peng, Goldstein, et al. 2024). The third generation of linear
recurrent models are based on more complex memory updating rule based on meta-learning, online learning, and/or
delta-rule, resulting in more expressive and effective models such as: Longhorn (B. Liu et al. 2024), Gated DeltaNet (S. Yang,
Kautz, and Hatamizadeh 2024), TTT (Yu Sun et al. 2024), and DeltaNet (S. Yang, B. Wang, Yu Zhang, et al. 2024). Our
LMM model can be seen as the next generation of such models, in which we incorporate the token flow into the memory
updating mechanism, having more powerful memory updating process. See Appendix C for a detailed discussion of
different recurrent models and Titans.
A.2 Transformer-based Architectures
Transformers. Transformers (Vaswani et al. 2017) as the de facto backbone for many deep learning models are based on
attention mechanism (Bahdanau 2014). They, however, suffer from quadratic computational cost, limiting their ability
to scale to long context window. To improve the memory consumption and throughput of softmax attention for longer
sequences, various studies focused on I/O aware implementations of attention (Dao 2024; Dao, D. Fu, et al. 2022), designing
more efficient attention mechanisms by sparsifying the attention matrix (B. Chen et al. 2021; Choromanski et al. 2021; Dai
et al. 2019; J. Dong et al. 2024; Roy et al. 2021), approximating the softmax (Arora et al. 2024), or developing kernel-based
(linear) attentions (Aksenov et al. 2024; Kacham, Mirrokni, and P. Zhong 2024; Schlag, Irie, and JÃ¼rgen Schmidhuber 2021;
S. Yang, B. Wang, Shen, et al. 2024).
Segment-based Transformers. Another line of research to improve the efficiency of Transformers is segment-based or
Chunk Transformers (Dai et al. 2019). The main drawback of chunk Transformers is that segments are fully separated and
so the context window is limited to the length of the chunks. To address this issue, various studies discuss the importance
of a memory so it can help the model to transfer information across chunks (Bulatov, Yuri Kuratov, et al. 2023; Bulatov,
Yury Kuratov, and Burtsev 2022; Feng et al. 2022; Hutchins et al. 2022; Rodkin et al. 2024; Z. Wang et al. 2019; Q. Wu
et al. 2020; Zancato et al. 2024). The key differences of Titans with these models are: (1) The memory in such models are
simple small size vectors, lacking expressive power to compress complex information; (2) The memory module lacks forget
mechanism, leading to a fast memory overflow; (3) only focus on momentary surprise, missing the information flow. More
specifically, recalling Recurrent Memory Transformers (RMT) (Bulatov, Yuri Kuratov, et al. 2023; Bulatov, Yury Kuratov,
and Burtsev 2022; Rodkin et al. 2024), one can treat Titans (MAC) as the generalization of RMT, where we use a neural
memory module instead of a vector-valued small size memory.
Memory for Large Language Models. Another interesting research direction has been to incorporate external memory
modules to LLMs after training (Z. He et al. 2024; Khandelwal et al. 2020; Y. Wang, Y. Gao, et al. 2024). Such models
are different from our approach as we incorporate the memory as a part of initial architecture and so we train it in
an end-to-end manner. Also, most of these explicit memory modules suffer from the same limitations as chunk-based
Transformers (mentioned above). For a detailed discussion of such models, we refer to the recent study of Y. Wang, Han,
et al. (2024).
25
A.3 Test Time Training and Fast Weight Programs
Memory Design and Augmentation with Memory. In the literature, a substantial research effort have been toward
designing memory modules that are capable of either memorizing the knowledge abstraction (e.g., persistent mem-
ory) (Sukhbaatar, Grave, et al. 2019), or memorizing the data-dependent information (also known as contextual memory),
through recurrence (Bulatov, Yury Kuratov, and Burtsev 2022; Rodkin et al. 2024; Zancato et al. 2024), Transformers (Berges
et al. 2024; Cetin et al. 2024; Feng et al. 2022; Le, Tran, and Venkatesh 2020; Munkhdalai, Faruqui, and Gopal 2024; J. Zhang
et al. 2024), gradient (Irie, CsordÃ¡s, and JÃ¼rgen Schmidhuber 2022; Munkhdalai, Sordoni, et al. 2019), or other learning
paradigms (Sukhbaatar, Weston, Fergus, et al. 2015; Weston, Chopra, and Bordes 2014). These memory models, however,
either (1) are based on momentary surprise, missing the data flow and events, (2) lack forget mechanisms to remove
the memory, leading to a fast memory overflow (3) are fixed-size shallow (matrix valued) memory, resulting in poor
performance in long context, and (4) are based on fixed parameters at test time, lacking test time adaption.
Fast Weight Programs. The idea of seeing linear layers as the key-value (associative) memory system backs to fast
weight programs, in which dynamic fast programs are incorporated into recurrent neural networks to serve as writable
memory (Schlag, Irie, and JÃ¼rgen Schmidhuber 2021; JH Schmidhuber 1992; JÃ¼rgen Schmidhuber 1993). The two learning
rules of Hebbian (Hebb 2005) and delta (Prados and Kak 1989) are the most popular learning rules for fast weight programs,
which have been extensively explored in various studies (Irie, Schlag, et al. 2021; Munkhdalai, Sordoni, et al. 2019;
Munkhdalai and H. Yu 2017; Schlag, Irie, and JÃ¼rgen Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, Kautz, and
Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. 2024). All these models, however, are based on momentary surprise,
missing the token flow in the sequences (see Section 3.1), and most of them lacks a forgetting gate, resulting in a poor
memory management.
Test Time Training. The key ideas of learning at test time or learning to learn (i.e., (Andrychowicz et al. 2016)) backs to
very early studies on local learning Bottou and Vapnik 1992, in which each test data sample is trained on its neighbors
before making a prediction (Gandelsman et al. 2022; H. Zhang et al. 2006). This approach further has shown promising
performance in vision tasks (Jain and Learned-Miller 2011; Mullapudi et al. 2019), mostly due to their ability to mitigate
out-of-distribution samples. The most similar studies to ours in this direction are MNM (Munkhdalai, Sordoni, et al. 2019)
and TTT-layer (Yu Sun et al. 2024), which we discussed the key differences in Appendix C.
B Language Modeling and Common-sense Reasoning Datasets
Following recent studies on linear recurrent models (Dao and Gu 2024; S. Yang, Kautz, and Hatamizadeh 2024; S. Yang,
B. Wang, Yu Zhang, et al. 2024), we use Wikitext (Merity et al. 2017), LMB (Paperno et al. 2016), PIQA (Bisk et al. 2020),
HellaSwag (Zellers et al. 2019), WinoGrande (Sakaguchi et al. 2021), ARC-easy (ARC-e) and ARC-challenge (ARC-c) (P.
Clark et al. 2018), SIQA (Sap et al. 2019), and BoolQ (C. Clark et al. 2019). Also, the baselines results for 400M models are
from the reported results by S. Yang, Kautz, and Hatamizadeh (2024).
C Long-term Memory Module (LMM) as a Sequence Model
In this section, we discuss how LMM as a sequence model is connected to modern linear recurrent models. For the sake
of simplicity, we start with a linear memory, where Mğ‘¡ =ğ‘Šğ‘¡ âˆˆ Rğ‘‘in Ã—ğ‘‘in . In this case, our objective function becomes
â„“(M;ğ‘¥ğ‘¡) =1
2 âˆ¥Mğ‘¡kğ‘¡ âˆ’vğ‘¡ âˆ¥22, in which we use gradient descent with momentum and weight decay for the optimization.
Accordingly, revisiting the recurrent formula in Equation 13:
Mğ‘¡ =diag (1 âˆ’ğ›¼ğ‘¡)Mğ‘¡ +ğ‘†ğ‘¡ (32)
ğ‘†ğ‘¡ =diag (ğœ‚ğ‘¡)ğ‘†ğ‘¡âˆ’1 âˆ’diag (ğœƒğ‘¡)(Mğ‘¡âˆ’1kâŠ¤ğ‘¡ kğ‘¡ âˆ’vâŠ¤ğ‘¡ kğ‘¡
) . (33)
LMM is Generalized Gated DeltaNet. As discussed by S. Yang, Kautz, and Hatamizadeh (2024), DeltaNet (S. Yang, B. Wang,
Yu Zhang, et al. 2024) can alternatively be interpreted as an online learning problem that optimizes the L =1
2 âˆ¥Sğ‘¡kğ‘¡ âˆ’vğ‘¡ âˆ¥22,
resulting in:
Sğ‘¡+1 =Sğ‘¡ âˆ’ğœƒğ‘¡âˆ‡L =Sğ‘¡
(I âˆ’ğœƒğ‘¡kğ‘¡kâŠ¤ğ‘¡
) +ğœƒğ‘¡vğ‘¡kâŠ¤ğ‘¡ . (34)
26
In this formulation, Gated DeltaNet is the same as above but with an additional weight decay term (S. Yang, Kautz, and
Hatamizadeh 2024). Comparing Equation 32 and Equation 34, we can see that setting ğœ‚ğ‘¡ =0 results in both formulations to
be equivalent. Accordingly, we can say LMM is generalizing the very recent study of Gated DeltaNet (S. Yang, Kautz, and
Hatamizadeh 2024) from three aspects:
â€¢Momentum-based Rule: The Delta Rule is based on momentary surprise, meaning that the flow of tokens cannot
affect the memory update rule. LMM, however, is based on a momentum rule, which consider both past and
momentary surprise.
â€¢Deep Memory: While Gated DeltaNet is limited to a linear (matrix-valued) memory as it requires finding the closed
recurrence form, LMM allows using deep memory module by using a gradient-based formulation, resulting in higher
expressive power.
â€¢Non-Linear Recurrence: While DeltaNet and Gated DeltaNet are based on linear recurrence, our LMM is using
inter-chunk non-linear recurrence and intra-chunk linear recurrence. This design allows LMM having a higher
expressive power.
Here, we discussed Gated DeltaNet as a sample of recent generation of recurrent models. Similar approaches such
as RWKV-7 (Peng 2021) are also using the same formulation and loss function, and so LMM is generalizing all such
models.
LMM is Generalized Longhorn. Similar to DeltaNet, Longhorn (B. Liu et al. 2024) uses the same loss function but it
derives the closed form using implicit online learning:
Sğ‘¡+1 =Sğ‘¡
(I âˆ’ğ›¿ğ‘¡kğ‘¡kâŠ¤ğ‘¡
) +ğ›¿ğ‘¡vğ‘¡kâŠ¤ğ‘¡ , (35)
where ğ›¿ğ‘¡ =ğœƒğ‘¡
1+ğœƒğ‘¡ kğ‘¡ kâŠ¤ğ‘¡
. It, however, lacks a forgetting gate, resulting in a faster memory overflow. Therefore, in addition two
the abovementioned aspects of (1) Momentum-based Rule, (2) Deep Memory, and (3) Non-Linear Recurrence, LMM has
the advantage of using an additional (4) Forget Gate, leading to a better memory management.
LMM is Generalized TTT Layer. To the best of our knowledge, TTT (Yu Sun et al. 2024), is the only modern linear
recurrent models with a gradient-based updating rule. In addition to different architectural designs and also objective
functions, our LMM has three key differences with presented TTT layers (Yu Sun et al. 2024):
1. Forgetting Mechanism: TTT layers are updating memory at each time, without having the chance to forget the
past data. Accordingly, when fixing the memory size, the model cannot manage the memory for long sequences. A
forget mechanism, such as LMMâ€™s, allows clearing the memory when very past information is not needed anymore.
We show that in a general case, this forget mechanism is equivalent to weight decay and provide a fast method to
incorporate it into the parallel training.
2. Momentum-based Update Rule: TTT layers are based on momentary surprise, meaning that the flow of tokens
cannot affect the memory update rule. LMM, however, is based on a momentum rule, which consider both past and
momentary surprise. See Section 3.1 for the motivation of this design.
3. Deep Memory: While TTT-layers allows for deeper memory, the advantages/disadvantages of such deeper memory
modules have not been experimentally evaluated.
To the best of our knowledge, our neural long-term memory module is the first linear recurrent model with momentum-
based update rule.
Finally, as a key difference with all the above and other recent linear recurrent studies, note that the hybrid variants of
modern linear modelsâ€“such as Griffin (De et al. 2024), DeltaNet (S. Yang, B. Wang, Yu Zhang, et al. 2024), Gated DeltaNet (S.
Yang, Kautz, and Hatamizadeh 2024), H3 (D. Y. Fu et al. 2023), Mamba2 (Dao and Gu 2024), Samba (Ren et al. 2024), etc.â€“all
are based on sequential layer-wise design. We present Titans to show how effectively one can incorporate such memory
modules into an architecture.
27
